import os
import uuid
import json
from typing import Optional, List, Dict, Any
from vox_unified.gatherer import Gatherer
from vox_unified.datalayer import DataLayer
from vox_unified.embeddings import get_ollama_embedding
from vox_unified.middleware import CacheLayer, TransformerLayer

class VoxManager:
    def __init__(self):
        self.datalayer = DataLayer()
        self.gatherer = Gatherer()
        self.cache = CacheLayer(self.datalayer.local.db_path)

    # --- PROJECT ---
    def project_create(self, path: str, name: Optional[str] = None):
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            err = f"‚ùå Error: Path {abs_path} does not exist."
            print(err)
            return err

        existing = self.datalayer.local.get_project_by_path(abs_path)
        if existing:
            msg = f"‚ÑπÔ∏è Project already registered: {existing['name']} (ID: {existing['id']})"
            print(msg)
            return existing['id']

        project_id = uuid.uuid4().hex[:16]
        project_name = name or os.path.basename(abs_path)
        
        self.datalayer.local.add_project(project_id, project_name, abs_path)
        msg = f"‚úÖ Project registered: {project_name} (ID: {project_id})"
        print(msg)
        return project_id

    def project_list(self):
        projects = self.datalayer.local.list_projects()
        if not projects:
            msg = "No projects registered."
            print(msg)
            return msg
        
        projects.sort(key=lambda x: x['name'])
        lines = [f"{'Alias':<6} {'ID':<18} {'Name':<20} {'Path'}", "-" * 80]
        env_lines = ["# Auto-generated by VOX CLI"]
        
        for i, p in enumerate(projects):
            alias = f"VX{i}"
            env_lines.append(f"export {alias}={p['id']}")
            lines.append(f"{alias:<6} {p['id']:<18} {p['name']:<20} {p['path']}")
        
        vox_home = os.environ.get("HOME", "")
        env_path = os.path.join(vox_home, ".vox2env")
        completion_block = """
# VOX Zsh Completion
_vox_completion() {
  local -a completions
  local -a response
  response=("${(@f)$(env COMP_WORDS="${words[*]}" COMP_CWORD=$((CURRENT-1)) _VOX_COMPLETE=complete_zsh vox)}")
  for type key descr in ${response}; do
    if [[ "$type" == "plain" ]]; then
      if [[ "$descr" == "_" ]]; then
        completions+=("$key")
      else
        completions+=("$key:$descr")
      fi
    fi
  done
  if [ -n "$completions" ]; then
    _describe -V unsorted completions -U
  fi
}
if command -v compdef > /dev/null; then
  compdef _vox_completion vox
fi
"""
        try:
            with open(env_path, "w") as f:
                f.write("\n".join(env_lines) + "\n")
                f.write(completion_block)
            lines.append("-" * 80)
            lines.append(f"‚ÑπÔ∏è  Aliases updated in {env_path}. Run 'source {env_path}' to apply.")
        except Exception:
            pass

        output = "\n".join(lines)
        print(output)
        return output

    def project_delete(self, project_id: str):
        self.datalayer.local.delete_project(project_id)
        self.datalayer.vector.delete_project_data(project_id)
        self.cache.invalidate(project_id)
        msg = f"‚úÖ Deleted project {project_id}"
        print(msg)
        return msg

    # --- INDEX ---
    def index_run(self, project_id: str, force: bool = False):
        project = self.datalayer.local.get_project(project_id)
        if not project:
            msg = f"‚ùå Project {project_id} not found."
            print(msg)
            return msg

        print(f"üöÄ Indexing {project['name']}...")
        
        # Systemic Fix: Always cleanup project data before re-indexing to prevent duplicates
        self.datalayer.vector.delete_project_data(project_id)
        self.cache.invalidate(project_id)

        # Re-cache tree
        self.get_project_tree(project_id)

        # Scan
        text_chunks, symbols = self.gatherer.scan_project(project['path'])
        
        # Local Docs
        local_docs = self.datalayer.local.list_documents(project_id)
        for doc in local_docs:
            text_chunks.append({
                "content": f"Title: {doc.get('title')}\n{doc['content']}",
                "type": doc['type'],
                "source": "local_db"
            })

        if symbols:
            print(f"Embedding {len(symbols)} symbols...")
            sym_embeddings = [get_ollama_embedding(f"{s.type} {s.name}\n{s.code[:200]}") for s in symbols]
            self.datalayer.vector.save_symbols(symbols, project_id, sym_embeddings)

        if text_chunks:
            print(f"Embedding {len(text_chunks)} text blocks...")
            text_embeddings = [get_ollama_embedding(chunk["content"]) for chunk in text_chunks]
            self.datalayer.vector.save_text_chunks(text_chunks, project_id, text_embeddings)
            
        return "‚úÖ Indexing complete."

    # --- SEARCH ---
    def search_run(self, query: str, project_id: Optional[str] = None, limit: int = 10):
        emb = get_ollama_embedding(query, is_query=True)
        
        # 1. Semantic Search
        text_hits = self.datalayer.vector.search_text(emb, project_id, limit)
        
        # 2. Symbolic Search
        sym_hits = self.datalayer.vector.search_symbols(query, emb, project_id, limit)
        
        # 3. Deduplication and Formatting
        print("=== Text/Docs ===")
        seen_text = set()
        for r in text_hits:
            if r.content not in seen_text:
                print(f"- [{r.relevance:.2f}] {r.content[:150].replace('\n', ' ')}...")
                seen_text.add(r.content)

        print("\n=== Code/Symbols ===")
        seen_sym = set()
        for r in sym_hits:
            if r.content not in seen_sym:
                print(f"- [{r.relevance:.2f}] {r.content.splitlines()[0]} ({r.source})")
                seen_sym.add(r.content)
                
        return "Search complete."

    # --- ASK ---
    def ask_run(self, question: str, project_id: str, model: Optional[str] = None, reset_history: bool = False):
        emb = get_ollama_embedding(question, is_query=True)
        text_hits = self.datalayer.vector.search_text(emb, project_id, 5)
        sym_hits = self.datalayer.vector.search_symbols(question, emb, project_id, 5)
        
        context = "### DOCUMENTATION / NOTES\n"
        for t in text_hits: context += f"{t.content}\n---\n"
        context += "\n### CODE SYMBOLS\n"
        for s in sym_hits: context += f"{s.content}\n---\n"
            
        import ollama
        resp = ollama.chat(model=model or "gemma3:4b-it-qat", messages=[{"role": "user", "content": f"Question: {question}\n\nContext:\n{context}"}])
        print(resp['message']['content'])
        return resp['message']['content']

    # --- DOCS ---
    def docs_add(self, project_id: str, content: str, title: Optional[str] = None, type: str = "note"):
        doc_id = self.datalayer.local.add_document(project_id, type, content, title)
        emb = get_ollama_embedding(f"Title: {title}\n{content}" if title else content)
        self.datalayer.vector.save_text_chunks([{"content": content, "type": type}], project_id, [emb])
        print(f"‚úÖ Saved doc (ID: {doc_id})")
        return doc_id

    def docs_list(self, project_id: str):
        docs = self.datalayer.local.list_documents(project_id)
        for d in docs:
            print(f"[{d['id']}] {d['type'].upper()}: {d['title'] or 'No Title'}")

    def get_project_tree(self, project_id: str):
        cached = self.cache.get(project_id, "file_tree")
        if cached: return cached
        proj = self.datalayer.local.get_project(project_id)
        if not proj: return "Not found"
        tree = []
        for root, dirs, files in os.walk(proj['path']):
            level = root.replace(proj['path'], '').count(os.sep)
            tree.append(f"{' ' * 4 * level}{os.path.basename(root)}/")
            for f in files: tree.append(f"{' ' * 4 * (level + 1)}{f}")
        res = "\n".join(tree)
        self.cache.set(project_id, "file_tree", res)
        return res

    def get_file_skeleton(self, project_id: str, file_path: str):
        proj = self.datalayer.local.get_project(project_id)
        if not proj: return "Not found"
        abs_path = os.path.join(proj['path'], file_path)
        if not os.path.exists(abs_path): return "Not found"
        with open(abs_path, 'r') as f:
            return TransformerLayer.generate_skeleton(f.read(), file_path)

    def server_start(self):
        from vox_unified.mcpserver import run
        run()
