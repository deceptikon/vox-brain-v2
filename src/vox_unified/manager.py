import os
import uuid
import json
from typing import Optional, List, Dict, Any
from vox_unified.gatherer import Gatherer
from vox_unified.datalayer import DataLayer
from vox_unified.embeddings import get_ollama_embedding
from vox_unified.middleware import CacheLayer, TransformerLayer

class VoxManager:
    def __init__(self):
        self.datalayer = DataLayer()
        self.gatherer = Gatherer()
        self.cache = CacheLayer(self.datalayer.local.db_path)

    # --- PROJECT ---
    def project_create(self, path: str, name: Optional[str] = None):
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            err = f"‚ùå Error: Path {abs_path} does not exist."
            print(err)
            return err

        existing = self.datalayer.local.get_project_by_path(abs_path)
        if existing:
            msg = f"‚ÑπÔ∏è Project already registered: {existing['name']} (ID: {existing['id']})"
            print(msg)
            return existing['id']

        project_id = uuid.uuid4().hex[:16]
        project_name = name or os.path.basename(abs_path)
        
        self.datalayer.local.add_project(project_id, project_name, abs_path)
        msg = f"‚úÖ Project registered: {project_name} (ID: {project_id})"
        print(msg)
        return project_id

    def project_list(self):
        projects = self.datalayer.local.list_projects()
        if not projects:
            msg = "No projects registered."
            print(msg)
            return msg
        
        # Sort by name for consistent aliasing
        projects.sort(key=lambda x: x['name'])

        lines = [f"{'Alias':<6} {'ID':<18} {'Name':<20} {'Path'}", "-" * 80]
        env_lines = ["# Auto-generated by VOX CLI"]
        
        for i, p in enumerate(projects):
            alias = f"VX{i}"
            env_lines.append(f"export {alias}={p['id']}")
            lines.append(f"{alias:<6} {p['id']:<18} {p['name']:<20} {p['path']}")
        
        # Write to ~/.vox2env
        vox_home = os.environ.get("HOME", "")
        env_path = os.path.join(vox_home, ".vox2env")
        
        # Add Completion block to env
        completion_block = """
# VOX Zsh Completion
_vox_completion() {
  eval $(env _TYPER_COMPLETE_ARGS="${words[1,$CURRENT]}" _VOX_COMPLETE=complete_zsh vox)
}
if command -v compdef > /dev/null; then
  compdef _vox_completion vox
fi
"""
        try:
            with open(env_path, "w") as f:
                f.write("\n".join(env_lines) + "\n")
                f.write(completion_block)
            lines.append("-" * 80)
            lines.append(f"‚ÑπÔ∏è  Aliases and Completion updated in {env_path}. Run 'source {env_path}' to apply.")
        except Exception as e:
            lines.append(f"‚ö†Ô∏è  Failed to write aliases: {e}")


        output = "\n".join(lines)
        print(output)
        return output

    def project_delete(self, project_id: str):
        self.datalayer.local.delete_project(project_id)
        self.datalayer.vector.delete_project_data(project_id)
        self.cache.invalidate(project_id)
        msg = f"‚úÖ Deleted project {project_id}"
        print(msg)
        return msg

    def project_stats(self, project_id: str):
        msg = "Stats not implemented."
        print(msg)
        return msg

    def get_project_tree(self, project_id: str):
        # 1. Try Cache
        cached = self.cache.get(project_id, "file_tree")
        if cached:
            return cached
            
        # 2. Build Tree
        proj = self.datalayer.local.get_project(project_id)
        if not proj: return "Project not found"
        
        tree = []
        for root, dirs, files in os.walk(proj['path']):
            level = root.replace(proj['path'], '').count(os.sep)
            indent = ' ' * 4 * level
            tree.append(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 4 * (level + 1)
            for f in files:
                tree.append(f"{subindent}{f}")
        
        tree_str = "\n".join(tree)
        self.cache.set(project_id, "file_tree", tree_str)
        return tree_str

    # --- INDEX ---
    def index_build(self, project_id: str, type: str = "all", force: bool = False):
        project = self.datalayer.local.get_project(project_id)
        if not project:
            msg = f"‚ùå Project {project_id} not found."
            print(msg)
            return msg

        print(f"üöÄ Building index for {project['name']}...")

        if force:
            self.datalayer.vector.delete_project_data(project_id)
            self.cache.invalidate(project_id)

        # Re-cache tree immediately
        self.get_project_tree(project_id)

        # Scan
        text_chunks, symbols = self.gatherer.scan_project(project['path'])
        
        # Local Docs
        local_docs = self.datalayer.local.get_all_documents_for_indexing(project_id)
        for doc in local_docs:
            text_chunks.append({
                "content": f"Title: {doc.get('title')}\n{doc['content']}",
                "type": doc['type'],
                "source": "local_db"
            })

        output_msgs = []
        # Embed Symbols
        if type in ["all", "symbolic"] and symbols:
            print(f"Embedding {len(symbols)} symbols...")
            sym_embeddings = []
            batch_texts = []
            for s in symbols:
                batch_texts.append(f"{s.type} {s.name}\n{s.code[:200]}")
            
            for txt in batch_texts:
                sym_embeddings.append(get_ollama_embedding(txt))

            self.datalayer.vector.save_symbols(symbols, project_id, sym_embeddings)
            msg = "‚úÖ Symbols indexed."
            print(msg)
            output_msgs.append(msg)

        # Embed Text
        if type in ["all", "semantic"] and text_chunks:
            print(f"Embedding {len(text_chunks)} text blocks...")
            text_embeddings = []
            for chunk in text_chunks:
                text_embeddings.append(get_ollama_embedding(chunk["content"]))
            
            self.datalayer.vector.save_text_chunks(text_chunks, project_id, text_embeddings)
            msg = "‚úÖ Text data indexed."
            print(msg)
            output_msgs.append(msg)
            
        return "\n".join(output_msgs)

    def index_update(self, project_id: str):
        return self.index_build(project_id, force=False)

    # --- DOCS ---
    def docs_add(self, project_id: str, content: Optional[str] = None, from_file: Optional[str] = None, type: str = "note", title: Optional[str] = None):
        if not content and not from_file:
            msg = "‚ùå Must provide content or file."
            print(msg)
            return msg

        final_content = content
        if from_file:
            try:
                with open(from_file, 'r') as f:
                    final_content = f.read()
            except Exception as e:
                msg = f"Error reading file: {e}"
                print(msg)
                return msg

        doc_id = self.datalayer.local.add_document(project_id, type, final_content, title, from_file)
        print(f"‚úÖ Doc saved locally (ID: {doc_id}).")

        print("Syncing to vector search...")
        text_to_embed = f"Title: {title}\n{final_content}" if title else final_content
        emb = get_ollama_embedding(text_to_embed)
        
        chunk = {
            "content": text_to_embed,
            "type": type
        }
        self.datalayer.vector.save_text_chunks([chunk], project_id, [emb])
        msg = f"‚úÖ Doc {doc_id} indexed for search."
        print(msg)
        return msg

    def docs_list(self, project_id: str):
        docs = self.datalayer.local.list_documents(project_id)
        lines = []
        for d in docs:
            line = f"[{d['id']}] {d['type'].upper()}: {d['title'] or 'No Title'} (Created: {d['created_at']})"
            lines.append(line)
        
        output = "\n".join(lines) if lines else "No docs found."
        print(output)
        return output

    def docs_get(self, project_id: str, doc_id: str):
        pass
        
    def docs_delete(self, project_id: str, doc_id: str):
        self.datalayer.local.delete_document(int(doc_id))
        msg = f"‚úÖ Document {doc_id} deleted locally. Run 'vox index build' to purge from search."
        print(msg)
        return msg

    # --- SEARCH ---
    def search_semantic(self, query: str, project_id: Optional[str] = None, limit: int = 10):
        emb = get_ollama_embedding(query, is_query=True)
        results = self.datalayer.vector.search_text(emb, project_id, limit)
        lines = []
        for r in results:
            line = f"- [{r.relevance:.2f}] {r.content[:100]}..."
            lines.append(line)
        
        output = "\n".join(lines) if lines else "No matches."
        print(output)
        return output

    def search_symbolic(self, query: str, project_id: Optional[str] = None, limit: int = 10):
        emb = get_ollama_embedding(query, is_query=True)
        results = self.datalayer.vector.search_symbols(query, emb, project_id, limit)
        lines = []
        for r in results:
            # Result content for symbols is typically the code snippet.
            # We want to show the name and path if possible.
            # Assuming Metadata contains the original symbol info or we parse it from content
            line = f"- [{r.relevance:.2f}] {r.content.splitlines()[0]} ({r.source})"
            lines.append(line)
            
        output = "\n".join(lines) if lines else "No matches."
        print(output)
        return output

    def search_auto(self, query: str, project_id: Optional[str] = None):
        print("=== Text/Docs ===")
        res1 = self.search_semantic(query, project_id, 5)
        print("\n=== Code/Symbols ===")
        res2 = self.search_symbolic(query, project_id, 5)
        return f"=== SEMANTIC ===\n{res1}\n\n=== SYMBOLIC ===\n{res2}"

    # --- ASK ---
    def ask_question(self, question: str, project_id: str, model: Optional[str] = None, reset_history: bool = False):
        emb = get_ollama_embedding(question, is_query=True)
        text_hits = self.datalayer.vector.search_text(emb, project_id, 5)
        sym_hits = self.datalayer.vector.search_symbols(question, emb, project_id, 5)

        
        context = "### DOCUMENTATION / NOTES\n"
        for t in text_hits:
            context += f"{t.content}\n---\n"
            
        context += "\n### CODE SYMBOLS\n"
        for s in sym_hits:
            context += f"{s.content}\n---\n"
            
        prompt = f"Question: {question}\n\nContext:\n{context}"
        
        import ollama
        resp = ollama.chat(model=model or "gemma3:4b-it-qat", messages=[{"role": "user", "content": prompt}])
        answer = resp['message']['content']
        print(answer)
        return answer
    
    # --- UTILS for Agents ---
    def get_file_skeleton(self, project_id: str, file_path: str):
        proj = self.datalayer.local.get_project(project_id)
        if not proj: return "Project not found"
        
        abs_path = os.path.join(proj['path'], file_path)
        if not os.path.exists(abs_path): return "File not found"
        
        try:
            with open(abs_path, 'r') as f:
                code = f.read()
            return TransformerLayer.generate_skeleton(code, file_path)
        except Exception as e:
            return f"Error reading file: {e}"

    # --- SERVER ---
    def server_start(self):
        from vox_unified.mcpserver import run
        run()

    def server_status(self):
        pass